{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/s7VPK1SPK2VbFnMaCKu7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandervaneck/DataScience/blob/main/DataScienceAsignment_Course4_SanderVanEck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJzqruViIUgX"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scikit-learn==1.0.0\n",
        "!pip install imbalanced-learn\n",
        "!pip install treeinterpreter\n",
        "!pip install dtreeviz\n",
        "!pip install graphviz\n",
        "!pip install opendatasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, make_scorer, explained_variance_score, precision_score, mean_absolute_error, classification_report, RocCurveDisplay, confusion_matrix,  recall_score, accuracy_score, roc_auc_score, precision_recall_curve, RocCurveDisplay, f1_score, mean_squared_error\n",
        "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split, learning_curve\n",
        "import math\n",
        "import seaborn\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "import io\n",
        "from google.colab import files\n",
        "from sklearn import metrics, utils, preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm as cm\n",
        "import plotly.graph_objs as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly import tools\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "\n",
        "%matplotlib inline\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "import pydot\n",
        "from IPython.display import Image\n",
        "from imblearn.combine import SMOTETomek\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv \n",
        "from sklearn.model_selection import HalvingGridSearchCV"
      ],
      "metadata": {
        "id": "GfLWfffsIqcD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "df_all = pd.read_csv(io.BytesIO(uploaded['resampled_df_05.csv']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "uOPRcwvlI2bP",
        "outputId": "e0021bc4-4eef-44c4-dd92-6805f9e40012"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2052c0e1-bfd8-42ed-8703-f988dc2cb45a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2052c0e1-bfd8-42ed-8703-f988dc2cb45a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving resampled_df_05.csv to resampled_df_05.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore Dataset"
      ],
      "metadata": {
        "id": "x3sqNBtXEzqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.describe()"
      ],
      "metadata": {
        "id": "WUw__78sE0_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for missing data among arrivals"
      ],
      "metadata": {
        "id": "-4A7QideE6Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numberOfNullArrivalDelayWhenArrivalData = sum((df_all.CRS_ARR_TIME >0) & (df_all.ARR_TIME >0) & (df_all.ARR_DELAY.isnull()))\n",
        "numberOfNoDataAndNotCancelledNorDiverted = sum(((df_all.CANCELLED==0) & (df_all.DIVERTED==0)) & (df_all.ARR_TIME.isnull()))\n",
        "\n",
        "print(f\"Number of flights with no arrival delay data, but can be derived: {numberOfNullArrivalDelayWhenArrivalData}\")\n",
        "print(f\"Number of flights that are missing data and were not cancelled, nor diverted: {numberOfNoDataAndNotCancelledNorDiverted}\")"
      ],
      "metadata": {
        "id": "k8rABtrBE6dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data transformation on entire dataset"
      ],
      "metadata": {
        "id": "_PHXusCbEEUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all['CANC_DIV'] = np.where(((df_all['CANCELLED'] == 1) | (df_all['DIVERTED'] == 1)), 1, 0)\n",
        "df_all.loc[(df_all.CRS_ARR_TIME >0) & (df_all.ARR_TIME >0) & (df_all.ARR_DELAY.isnull()), \"ARR_DELAY\"] = df_all['CRS_ARR_TIME'] - df_all['ARR_TIME']\n",
        "\n",
        "df_all['DATES'] =  pd.to_datetime(df_all.FL_DATE)\n",
        "\n",
        "# Month variable\n",
        "df_all['month'] = pd.to_datetime(df_all['DATES']).dt.month\n",
        "# Day variable\n",
        "df_all['day'] = pd.to_datetime(df_all['DATES']).dt.day\n",
        "# Weekday variable\n",
        "df_all['weekday'] = pd.to_datetime(df_all['DATES']).dt.weekday \n",
        "\n",
        "df_all['scheduled_hour'] = 0\n",
        "df_all.loc[df_all['CRS_DEP_TIME'] < 100, 'scheduled_hour'] = 1\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 100) & (df_all['CRS_DEP_TIME'] < 200), 'scheduled_hour'] = 2\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 200) & (df_all['CRS_DEP_TIME'] < 300), 'scheduled_hour'] = 3\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 300) & (df_all['CRS_DEP_TIME'] < 400), 'scheduled_hour'] = 4\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 400) & (df_all['CRS_DEP_TIME'] < 500), 'scheduled_hour'] = 5\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 500) & (df_all['CRS_DEP_TIME'] < 600), 'scheduled_hour'] = 6\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 600) & (df_all['CRS_DEP_TIME'] < 700), 'scheduled_hour'] = 7\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 700) & (df_all['CRS_DEP_TIME'] < 800), 'scheduled_hour'] = 8\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 800) & (df_all['CRS_DEP_TIME'] < 900), 'scheduled_hour'] = 9\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 900) & (df_all['CRS_DEP_TIME'] < 1000), 'scheduled_hour'] = 10\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1000) & (df_all['CRS_DEP_TIME'] < 1100), 'scheduled_hour'] = 11\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1100) & (df_all['CRS_DEP_TIME'] < 1200), 'scheduled_hour'] = 12\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1200) & (df_all['CRS_DEP_TIME'] < 1300), 'scheduled_hour'] = 13\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1300) & (df_all['CRS_DEP_TIME'] < 1400), 'scheduled_hour'] = 14\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1400) & (df_all['CRS_DEP_TIME'] < 1500), 'scheduled_hour'] = 15\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1500) & (df_all['CRS_DEP_TIME'] < 1600), 'scheduled_hour'] = 16\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1600) & (df_all['CRS_DEP_TIME'] < 1700), 'scheduled_hour'] = 17\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1700) & (df_all['CRS_DEP_TIME'] < 1800), 'scheduled_hour'] = 18\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1800) & (df_all['CRS_DEP_TIME'] < 1900), 'scheduled_hour'] = 19\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 1900) & (df_all['CRS_DEP_TIME'] < 2000), 'scheduled_hour'] = 20\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 2000) & (df_all['CRS_DEP_TIME'] < 2100), 'scheduled_hour'] = 21\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 2100) & (df_all['CRS_DEP_TIME'] < 2200), 'scheduled_hour'] = 22\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 2200) & (df_all['CRS_DEP_TIME'] < 2300), 'scheduled_hour'] = 23\n",
        "df_all.loc[(df_all['CRS_DEP_TIME'] >= 2300) & (df_all['CRS_DEP_TIME'] < 2400), 'scheduled_hour'] = 24\n",
        "\n",
        "day = [1\t,15\t,14\t,19\t,17\t,1\t,17\t,25\t,13\t,28\t,17\t,4\t,3\t,8\t,31\t,11\t,12\t,22\t,23\t,24\t,25\t,31\t]\n",
        "month = [1\t,1\t,2\t,2\t,3\t,4\t,4\t,4\t,5\t,5\t,6\t,7\t,9\t,10\t,10\t,11\t,11\t,11\t,11\t,12\t,12\t,12\t]\n",
        "year = [2018, 2018, 2018, 2018, 2018, 2018\t,2018,\t2018,2018, 2018, 2018\t,2018,\t2018,2018, 2018, 2018\t,2018,\t2018,2018, 2018, 2018\t,2018]\n",
        "holidays = pd.DataFrame({'year': year,\n",
        "                   'month': month,\n",
        "                   'day': day\n",
        "                   })\n",
        "\n",
        "df_all['is_holiday'] = np.where((df_all['month'].isin(holidays.month)) & (df_all['day'].isin(holidays.day)),1,0)\n",
        "\n",
        "def my_scaler(delay_df, from_col_name, to_col_name):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = scaler.fit_transform(pd.DataFrame(delay_df[from_col_name],index=delay_df.index))\n",
        "    scaled_delay_df = pd.DataFrame(scaled, columns=['scale_temp'], index=delay_df.index)\n",
        "    delay_df[to_col_name] = scaled_delay_df['scale_temp']\n",
        "    \n",
        "df_all['origin_canc_perc'] = df_all.groupby('ORIGIN')['CANC_DIV'].transform('mean')\n",
        "my_scaler(df_all,'origin_canc_perc','scaled_origin_airport_score')\n",
        "\n",
        "df_all['dest_canc_perc'] = df_all.groupby('DEST')['CANC_DIV'].transform('mean')\n",
        "my_scaler(df_all,'dest_canc_perc','scaled_dest_airport_score')\n",
        "\n",
        "df_all['airline_canc_perc'] = df_all.groupby('OP_CARRIER')['CANC_DIV'].transform('mean')\n",
        "my_scaler(df_all,'airline_canc_perc','scaled_airline_score')\n",
        "\n",
        "df_all['weekday_canc_prcnt'] = df_all.groupby('weekday')['CANC_DIV'].transform('mean')\n",
        "my_scaler(df_all,'weekday_canc_prcnt','scaled_weekday_delay_score')\n",
        "\n",
        "df_all['month_canc_prcnt'] = df_all.groupby('month')['CANC_DIV'].transform('mean')\n",
        "my_scaler(df_all,'month_canc_prcnt','scaled_month_delay_score')\n"
      ],
      "metadata": {
        "id": "1ZuCEYipgSOj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define dataset for delays and calculate standard normally scaled scores"
      ],
      "metadata": {
        "id": "JGfTxXl7EKKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delay_df = df_all[(df_all['CANCELLED']==0) & (df_all['DIVERTED']==0)]\n",
        "\n",
        "delay_df[\"ARRIVAL_DELAYED\"] = np.where(delay_df['ARR_DELAY'] > 15, 1, 0)\n",
        "delay_df['log_delay'] = np.where(delay_df['ARR_DELAY'] > 0, np.log1p(delay_df.ARR_DELAY) + 1, -1 * ( 1+ np.log1p(delay_df.ARR_DELAY * -1)))\n",
        "    \n",
        "delay_df['mean_origin_airport_delay'] = delay_df.groupby('ORIGIN')['ARR_DELAY'].transform('mean')\n",
        "delay_df['airport_origin_delay_prcnt'] = delay_df.groupby('ORIGIN')['ARRIVAL_DELAYED'].transform('mean')\n",
        "delay_df['airport_origin_delay_score'] = delay_df['mean_origin_airport_delay'] * delay_df['airport_origin_delay_prcnt']\n",
        "\n",
        "my_scaler(delay_df,'airport_origin_delay_score','scaled_origin_airport_score')\n",
        "\n",
        "delay_df['mean_dest_airport_delay'] = delay_df.groupby('DEST')['ARR_DELAY'].transform('mean')\n",
        "delay_df['airport_dest_delay_prcnt'] = delay_df.groupby('DEST')['ARRIVAL_DELAYED'].transform('mean')\n",
        "delay_df['airport_dest_delay_score'] = delay_df['mean_dest_airport_delay'] * delay_df['airport_dest_delay_prcnt']\n",
        "\n",
        "my_scaler(delay_df,'airport_dest_delay_score','scaled_dest_airport_score')\n",
        "\n",
        "delay_df['mean_airline_delay'] = delay_df.groupby('OP_CARRIER')['ARR_DELAY'].transform('mean')\n",
        "delay_df['airline_delay_prcnt'] = delay_df.groupby('OP_CARRIER')['ARRIVAL_DELAYED'].transform('mean')\n",
        "delay_df['airline_delay_score'] = delay_df['mean_airline_delay'] * delay_df['airline_delay_prcnt']\n",
        "my_scaler(delay_df,'airline_delay_score','scaled_airline_score')\n",
        "\n",
        "delay_df['mean_weekday_delay'] = delay_df.groupby('weekday')['ARR_DELAY'].transform('mean')\n",
        "delay_df['weekday_delay_prcnt'] = delay_df.groupby('weekday')['ARRIVAL_DELAYED'].transform('mean')\n",
        "delay_df['weekday_delay_score'] = delay_df['mean_weekday_delay'] * delay_df['weekday_delay_prcnt']\n",
        "my_scaler(delay_df,'weekday_delay_prcnt','scaled_weekday_delay_score')\n",
        "\n",
        "delay_df['mean_month_delay'] = delay_df.groupby('month')['ARR_DELAY'].transform('mean')\n",
        "delay_df['month_delay_prcnt'] = delay_df.groupby('month')['ARRIVAL_DELAYED'].transform('mean')\n",
        "delay_df['month_delay_score'] = delay_df['mean_month_delay'] * delay_df['month_delay_prcnt']\n",
        "my_scaler(delay_df,'month_delay_prcnt','scaled_month_delay_score')\n",
        "\n",
        "delay_df['mean_day_delay'] = delay_df.groupby('day')['ARR_DELAY'].transform('mean')\n",
        "delay_df['day_delay_prcnt'] = delay_df.groupby('day')['ARRIVAL_DELAYED'].transform('mean')\n",
        "delay_df['day_delay_score'] = delay_df['mean_day_delay'] * delay_df['day_delay_prcnt']\n",
        "my_scaler(delay_df,'day_delay_prcnt','scaled_day_delay_score')"
      ],
      "metadata": {
        "id": "vwrNbBYRD_NY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "numberOfNullArrivalDelayWhenArrivalData = sum((df_all.CRS_ARR_TIME >0) & (df_all.ARR_TIME >0) & (df_all.ARR_DELAY.isnull()))\n",
        "numberOfNoDataAndNotCancelledNorDiverted = sum(((delay_df.CANCELLED==0) & (delay_df.DIVERTED==0)) & (delay_df.ARR_TIME.isnull()))\n",
        "\n",
        "print(f\"Number of flights with no arrival delay data, but can be derived: {numberOfNullArrivalDelayWhenArrivalData}\")\n",
        "print(f\"Number of flights that are missing data and were not cancelled, nor diverted: {numberOfNoDataAndNotCancelledNorDiverted}\")"
      ],
      "metadata": {
        "id": "u-6ehVP_FafP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore datasets"
      ],
      "metadata": {
        "id": "K2-MbHHCEhoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hours = df_all.scheduled_hour.value_counts()\n",
        "months = df_all.month.value_counts()\n",
        "weekdays = df_all.weekday.value_counts()\n",
        "carriers = df_all.OP_CARRIER.unique()\n",
        "origins = df_all.ORIGIN.unique()\n",
        "destinations = df_all.DEST.unique()\n",
        "cancelled = df_all.CANCELLED.value_counts()\n",
        "div = df_all.DIVERTED.value_counts()\n",
        "\n",
        "display(cancelled)\n",
        "display(div)\n",
        "display(months)\n",
        "display(weekdays)\n",
        "print(f\"\\nThe number of different carriers equals (in the train set): {len(carriers)}\")\n",
        "print(f\"\\nThe number of different destinations equals (in the train set): {len(destinations)}\")\n",
        "print(f\"\\nThe number of different origins equals (in the train set): {len(origins)}\")"
      ],
      "metadata": {
        "id": "pJc2WjnkEktB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_var = ['OP_CARRIER','ORIGIN', 'DEST', 'scheduled_hour', 'month', 'is_holiday', 'weekday']"
      ],
      "metadata": {
        "id": "cBGpNbrDIwyT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for var in cat_var:\n",
        "  df_all[var].value_counts().plot(figsize=(20, 10), kind=\"bar\",title=f\"Frequency distribution variable {var}\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JiZt5Tf7IpjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split both datasets into train, validation and test sets"
      ],
      "metadata": {
        "id": "dEf5U4YkEcet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "train_df, aux_df = train_test_split(delay_df, train_size=.5, random_state=seed)\n",
        "validation_df, test_df = train_test_split(aux_df, train_size=.5, random_state=seed)\n",
        "\n",
        "canc_train_df, canc_aux_df = train_test_split(df_all, train_size=.5, random_state=seed)\n",
        "canc_validation_df, canc_test_df = train_test_split(canc_aux_df, train_size=.5, random_state=seed)"
      ],
      "metadata": {
        "id": "VZtImQFqJwpW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['ARR_DELAY'].describe(percentiles=[0.95,0.975, 0.98, 0.99, 0.995, 1.0])"
      ],
      "metadata": {
        "id": "OTMYD-WuWN-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove outliers\n",
        "toppart_df = (train_df['ARR_DELAY'] > train_df['ARR_DELAY'].quantile(0.999))\n",
        "train_df = train_df.loc[~toppart_df,:]\n",
        "\n",
        "#remove infrequent airports and airlines\n",
        "train_df['origin_count'] = train_df.groupby('ORIGIN')['ORIGIN'].transform('count')\n",
        "train_df['dest_count'] = train_df.groupby('DEST')['DEST'].transform('count')\n",
        "train_df['air_count'] = train_df.groupby('OP_CARRIER')['OP_CARRIER'].transform('count')\n",
        "\n",
        "minimum_ori_df=train_df['origin_count'].quantile(0.05)\n",
        "minimum_dest_df=train_df['dest_count'].quantile(0.05)\n",
        "minimum_air_df=train_df['air_count'].quantile(0.05)\n",
        "\n",
        "train_df = train_df[(((train_df['origin_count']>minimum_ori_df) & (train_df['dest_count']>minimum_dest_df)) & (train_df['air_count']>minimum_air_df))]"
      ],
      "metadata": {
        "id": "5I4Q6QjYhK70"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove infrequent airports and airlines\n",
        "canc_train_df['origin_count'] = canc_train_df.groupby('ORIGIN')['ORIGIN'].transform('count')\n",
        "canc_train_df['dest_count'] = canc_train_df.groupby('DEST')['DEST'].transform('count')\n",
        "canc_train_df['air_count'] = canc_train_df.groupby('OP_CARRIER')['OP_CARRIER'].transform('count')\n",
        "\n",
        "minimum_ori_df=canc_train_df['origin_count'].quantile(0.05)\n",
        "minimum_dest_df=canc_train_df['dest_count'].quantile(0.05)\n",
        "minimum_air_df=canc_train_df['air_count'].quantile(0.05)\n",
        "\n",
        "canc_train_df = canc_train_df[(((canc_train_df['origin_count']>minimum_ori_df) & (canc_train_df['dest_count']>minimum_dest_df)) & (canc_train_df['air_count']>minimum_air_df))]"
      ],
      "metadata": {
        "id": "RGhbF_6eV1sU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_balancing(df, var):\n",
        "  target_count = df[var].value_counts()\n",
        "  target_count.plot(kind='bar', title=var);\n",
        "  print(f\"{len(df.loc[df[var] == 1]) / len(df)*100}%\")\n"
      ],
      "metadata": {
        "id": "2auF5ePhMRTk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_balancing(train_df, 'ARRIVAL_DELAYED')"
      ],
      "metadata": {
        "id": "aISjhmUFMdFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_balancing(canc_train_df, 'CANC_DIV')"
      ],
      "metadata": {
        "id": "E6s7s_ymSyBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_factors = ['scaled_weekday_delay_score', 'scaled_month_delay_score', 'scaled_airline_score', 'scaled_origin_airport_score', 'scheduled_hour', 'scaled_dest_airport_score', 'is_holiday']"
      ],
      "metadata": {
        "id": "UkVwOfVmJ3l5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_trained = train_df.loc[:,final_factors]\n",
        "y_bin_train = train_df.loc[:,'ARRIVAL_DELAYED']\n",
        "y_log_train = train_df.loc[:,'log_delay']\n",
        "y_delay_train = train_df.loc[:,'ARR_DELAY']\n",
        "X_val = validation_df.loc[:,final_factors]\n",
        "y_bin_val = validation_df.loc[:,'ARRIVAL_DELAYED']\n",
        "y_log_val = validation_df.loc[:,'log_delay']\n",
        "y_delay_val = validation_df.loc[:,'ARR_DELAY']\n",
        "X_test = test_df.loc[:,final_factors]\n",
        "y_bin_test = test_df.loc[:,'ARRIVAL_DELAYED']\n",
        "y_log_test = test_df.loc[:,'log_delay']\n",
        "y_delay_test = test_df.loc[:,'ARR_DELAY']"
      ],
      "metadata": {
        "id": "PjlO4UaIJ6X-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_canc_factors = ['scaled_weekday_delay_score', 'scaled_month_delay_score', 'scaled_airline_score', 'scaled_origin_airport_score', 'scheduled_hour', 'scaled_dest_airport_score', 'is_holiday']\n",
        "X_trained_canc = canc_train_df.loc[:,final_canc_factors]\n",
        "y_canc_trained = canc_train_df.loc[:,'CANC_DIV']\n",
        "\n",
        "X_val_canc = canc_validation_df.loc[:,final_canc_factors]\n",
        "y_canc_val = canc_validation_df.loc[:,'CANC_DIV']\n",
        "\n",
        "X_test_canc = canc_test_df.loc[:,final_canc_factors]\n",
        "y_canc_test = canc_test_df.loc[:,'CANC_DIV']"
      ],
      "metadata": {
        "id": "X6llg1fUDkeH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore independent factors"
      ],
      "metadata": {
        "id": "MtybSL-Qv71f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delay_df.loc[:,final_factors].describe()"
      ],
      "metadata": {
        "id": "zjUzdYNBskeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.loc[:,final_canc_factors].describe()"
      ],
      "metadata": {
        "id": "qjDBhe8HsMrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balance Training Sets"
      ],
      "metadata": {
        "id": "UyAj-0RLv_Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Balance training set\n",
        "print('Resampled dataset shape %s' % Counter(y_bin_train))\n",
        "smt = SMOTETomek(random_state=10)\n",
        "X_train, y_train = smt.fit_resample(X_trained, y_bin_train)\n",
        "print('Resampled dataset shape %s' % Counter(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggIXDsyEiRcU",
        "outputId": "2bd8ac94-cf9d-4604-f153-5adbfadc09ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled dataset shape Counter({0: 121219, 1: 27411})\n",
            "Resampled dataset shape Counter({0: 112795, 1: 112795})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Balance training set\n",
        "print('Resampled dataset shape %s' % Counter(y_canc_trained))\n",
        "smt = SMOTETomek(random_state=10)\n",
        "X_canc_train, y__canc_train = smt.fit_resample(X_trained_canc, y_canc_trained)\n",
        "print('Resampled dataset shape %s' % Counter(y__canc_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwMp5itnD9X_",
        "outputId": "c6f75df3-998e-4abf-ac45-595c92e611f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled dataset shape Counter({0: 148718, 1: 2878})\n",
            "Resampled dataset shape Counter({0: 148222, 1: 148222})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See if datasets are balanced"
      ],
      "metadata": {
        "id": "2sw7sMtxwNW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts().plot(figsize=(20, 10), kind=\"bar\",title=f\"ARRIVAL_DELAYED\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pehXMz2rdyX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y__canc_train.value_counts().plot(figsize=(20, 10), kind=\"bar\",title=f\"ARRIVAL_DELAYED\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SWGhsK-UuiWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize correlation matrices"
      ],
      "metadata": {
        "id": "gZaHfWlUwCar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(seaborn.heatmap(X_train.corr(method=\"pearson\")) ) "
      ],
      "metadata": {
        "id": "q7Uzol39Q5hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(seaborn.heatmap(X_canc_train.corr(method=\"pearson\")))  \n"
      ],
      "metadata": {
        "id": "NdG4RoOwSWIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize distribution of delays"
      ],
      "metadata": {
        "id": "HjF_55M9wRGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "ax = train_df.loc[:,'ARR_DELAY'].plot.hist(bins=200)\n",
        "ax.set_xlim(-50, 100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fQtXY1azJNYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot delays over time"
      ],
      "metadata": {
        "id": "PPvYTaKUwT3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(30,5))\n",
        "\n",
        "ax1.plot(train_df.groupby('DATES')['ARRIVAL_DELAYED'].mean(),data=train_df, color='g')\n",
        "ax1.set_xlabel('Days of the year')\n",
        "ax1.set_ylabel('late %', color='g')\n",
        "ax1.set_xlim(min(train_df['DATES']), max(train_df['DATES']))\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(train_df.groupby('DATES')['ARR_DELAY'].mean(),data=train_df, color = \"r\")\n",
        "ax2.set_ylabel('Time of delay', color = \"r\")"
      ],
      "metadata": {
        "id": "MjPOYbLoJLlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore delays per category"
      ],
      "metadata": {
        "id": "uqANjRapwXpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for var in cat_var:\n",
        "    fig = plt.figure(figsize=(20,6))\n",
        "    ax1 = fig.gca()\n",
        "\n",
        "    train_df.groupby(var)['ARR_DELAY','ARRIVAL_DELAYED'].mean().sort_values(by='ARR_DELAY', ascending=False).plot(kind='bar', secondary_y='ARRIVAL_DELAYED', ax=ax1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ySeWGxhVvsY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore cancellations over time"
      ],
      "metadata": {
        "id": "hHoU74Kh0HRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(30,5))\n",
        "\n",
        "ax1.plot(canc_train_df.groupby('DATES')['CANC_DIV'].mean(),data=train_df, color='g')\n",
        "ax1.set_xlabel('Days of the year')\n",
        "ax1.set_ylabel('cancelled/diverted %', color='g')\n",
        "ax1.set_xlim(min(train_df['DATES']), max(train_df['DATES']))"
      ],
      "metadata": {
        "id": "KhuZNaIm0GUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore cancellations per category"
      ],
      "metadata": {
        "id": "Tu5-7UIczDP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for var in cat_var:\n",
        "    fig = plt.figure(figsize=(20,6))\n",
        "    ax1 = fig.gca()\n",
        "\n",
        "    canc_train_df.groupby(var)['CANC_DIV'].mean().plot(kind='bar', ax=ax1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T-_UbAVpzCL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation**"
      ],
      "metadata": {
        "id": "YHJFZ_cQSY4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define Models for delays on training sets\n",
        "2. Plot and evaluate results"
      ],
      "metadata": {
        "id": "c1d8rArdQYDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tree based on cost_FP, cost_FN\n",
        "ctree = tree.DecisionTreeClassifier()\n",
        "rtree = tree.DecisionTreeRegressor()\n",
        "model_bin_ctree = ctree.fit(X_train, y_train)\n",
        "model_reg_tree = rtree.fit(X_trained, y_log_train)\n",
        "\n",
        "#Random Forest\n",
        "rfc = RandomForestClassifier()\n",
        "rrfc = RandomForestRegressor()\n",
        "model_bin_rfc = rfc.fit(X_train, y_train)\n",
        "model_reg_rfc = rrfc.fit(X_trained, y_log_train)\n",
        "\n",
        "#Bagged tree\n",
        "clb = BaggingClassifier()\n",
        "model_bin_clb = clb.fit(X_train, y_train)\n",
        "rclb = BaggingRegressor()\n",
        "model_reg_clb = rclb.fit(X_trained, y_log_train)\n",
        "\n",
        "#Gradient Boosting\n",
        "gb_clf = GradientBoostingClassifier()\n",
        "model_bin_gb_clf = gb_clf.fit(X_train, y_train)\n",
        "rgb_clf = GradientBoostingRegressor()\n",
        "model_reg_gb_clb = rgb_clf.fit(X_trained, y_log_train)"
      ],
      "metadata": {
        "id": "lCeEaC-4idwR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions to plot evaluation results"
      ],
      "metadata": {
        "id": "KjdcJROVTXlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc(y, hat_prob_y):\n",
        "    RocCurveDisplay.from_predictions(\n",
        "        y,\n",
        "        hat_prob_y,\n",
        "        name=\"ROC curve\",\n",
        "        color=\"darkorange\",\n",
        "    )\n",
        "    plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
        "    plt.axis(\"square\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC curve\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(hat_y, y, target_names):\n",
        "    matrix = confusion_matrix(y, hat_y)  \n",
        "    sns.heatmap(matrix.T, square=True, annot=True, fmt=\"d\", cbar=False,\n",
        "    xticklabels=target_names, yticklabels=target_names)\n",
        "    plt.xlabel(\"true label\")\n",
        "    plt.ylabel(\"predicted label\")\n",
        "\n",
        "def estimate_cost(hat_y, y, cost_FP, cost_FN):\n",
        "    return  np.sum(np.multiply(hat_y, (1 - y)) * cost_FP) + np.sum(np.multiply((1 - hat_y), y) * cost_FN)\n",
        "\n",
        "def plot_to_evaluate(y, y_hat, title):\n",
        "  print(f\"{title} costs: {estimate_cost(y_hat, y, 1, 1)}\")\n",
        "  print(f\"{title} MSE:{mean_squared_error(y, y_hat)}\")\n",
        "  print(f\"{title} RMSE:{mean_squared_error(y, y_hat)**0.5}\")\n",
        "  print(f\"report: {classification_report(y, y_hat)}\")\n",
        "  print(f\"{title} accuracy score: {accuracy_score(y, y_hat)}\")\n",
        "\n",
        "def print_results(y, y_hat, title):\n",
        "  print(f\"{title} MAE:{mean_absolute_error(y, y_hat)}\")\n",
        "  print(f\"{title} MSE:{mean_squared_error(y, y_hat)}\")\n",
        "  print(f\"{title} RMSE:{mean_squared_error(y, y_hat)**0.5}\")\n",
        "  print(f\"{title} Explained Variance Score:{explained_variance_score(y, y_hat)}\")\n",
        "  print(f\"{title} R2: {r2_score(y, y_hat)}\")"
      ],
      "metadata": {
        "id": "B6vFSrADTLoq"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot and evaluate them on validation sets"
      ],
      "metadata": {
        "id": "h-mS-Oc2TadB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_to_evaluate(y_bin_val, model_bin_ctree.predict(X_val),  \"Simple decision tree\")\n",
        "plot_to_evaluate(y_bin_val, model_bin_rfc.predict(X_val),\"Random Forest\")\n",
        "plot_to_evaluate(y_bin_val, model_bin_clb.predict(X_val),  \"Bagging Tree\")\n",
        "plot_to_evaluate(y_bin_val, model_bin_gb_clf.predict(X_val),  \"Gradient Boosting\")\n",
        "\n",
        "print_results(y_log_val, model_reg_tree.predict(X_val), \"Regression Tree\")\n",
        "print_results(y_log_val, model_reg_rfc.predict(X_val), \"Random Forest\")\n",
        "print_results(y_log_val, model_reg_clb.predict(X_val), \"Bagging regressor\")\n",
        "print_results(y_log_val, model_reg_gb_clb.predict(X_val), \"Regression Boost\")"
      ],
      "metadata": {
        "id": "RJGfrLdxTVMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_bin_val, model_bin_ctree.predict(X_val), [0, 1])"
      ],
      "metadata": {
        "id": "Zhd7c_5pHsoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_bin_val, model_bin_rfc.predict(X_val), [0, 1])"
      ],
      "metadata": {
        "id": "nqdHNIz6Hyg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_bin_val, model_bin_clb.predict(X_val), [0, 1])"
      ],
      "metadata": {
        "id": "0cn4xg-yHzdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_bin_val, model_bin_gb_clf.predict(X_val), [0, 1])"
      ],
      "metadata": {
        "id": "gnpuGOXXH0wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs1 = model_bin_ctree.predict_proba(X_val)[:,1]\n",
        "precision1, recall1, _ = precision_recall_curve(y_bin_val, probs1)\n",
        "probs2 = model_bin_rfc.predict_proba(X_val)[:,1]\n",
        "precision2, recall2, _ = precision_recall_curve(y_bin_val, probs2)\n",
        "probs3 = model_bin_clb.predict_proba(X_val)[:,1]\n",
        "precision3, recall3, _ = precision_recall_curve(y_bin_val, probs3)\n",
        "probs4 = model_bin_gb_clf.predict_proba(X_val)[:,1]\n",
        "precision4, recall4, _ = precision_recall_curve(y_bin_val, probs4)\n",
        "no_skill = len(y_bin_val[y_bin_val==1]) / len(y_bin_val)\n",
        "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "pyplot.plot(recall1, precision1, marker='.', label='Simple')\n",
        "pyplot.plot(recall2, precision2, marker='.', label='RF')\n",
        "pyplot.plot(recall3, precision3, marker='.', label='Bagged')\n",
        "pyplot.plot(recall4, precision4, marker='.', label='Gradient')\n",
        "pyplot.xlabel('Recall')\n",
        "pyplot.ylabel('Precision')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "nre0eyoNPn0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Develop models for cancelled/diverted flights based on training sets and train them on validation set"
      ],
      "metadata": {
        "id": "cihv0BI1Q3Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tree based on cost_FP, cost_FN\n",
        "ctree = tree.DecisionTreeClassifier()\n",
        "    # class_weight=class_weight)\n",
        "model_canc_bin_ctree = ctree.fit(X_canc_train, y__canc_train)\n",
        "\n",
        "#Random Forest\n",
        "rfc = RandomForestClassifier()\n",
        "# class_weight=class_weight)\n",
        "model_canc_bin_rfc = rfc.fit(X_canc_train, y__canc_train)\n",
        "\n",
        "#Bagged tree\n",
        "clb = BaggingClassifier()\n",
        "model_canc_bin_clb = clb.fit(X_canc_train, y__canc_train)\n",
        "\n",
        "#Gradient Boosting\n",
        "gb_clf = GradientBoostingClassifier()\n",
        "model_canc_bin_gb_clf = gb_clf.fit(X_canc_train, y__canc_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "x3isrneIEOj3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot and evaluate"
      ],
      "metadata": {
        "id": "Q2EVkpCmTgks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_to_evaluate(y_canc_val, model_canc_bin_ctree.predict(X_val_canc), model_canc_bin_ctree.predict_proba(X_val_canc)[:,1], \"Simple decision tree\")\n",
        "plot_to_evaluate(y_canc_val, model_canc_bin_rfc.predict(X_val_canc), model_canc_bin_rfc.predict_proba(X_val_canc)[:,1], \"Random Forest\")\n",
        "plot_to_evaluate(y_canc_val, model_canc_bin_clb.predict(X_val_canc), model_canc_bin_clb.predict_proba(X_val_canc)[:,1], \"Bagging Tree\")\n",
        "plot_to_evaluate(y_canc_val, model_canc_bin_gb_clf.predict(X_val_canc), model_canc_bin_gb_clf.predict_proba(X_val_canc)[:,1], \"Gradient Boosting\")"
      ],
      "metadata": {
        "id": "mV5y58QFTfwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_canc_val, model_canc_bin_ctree.predict(X_val_canc), [0, 1])"
      ],
      "metadata": {
        "id": "gkHX9dZbRbbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_canc_val, model_canc_bin_rfc.predict(X_val_canc), [0, 1])"
      ],
      "metadata": {
        "id": "at7HlGOjRcfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_canc_val, model_canc_bin_clb.predict(X_val_canc), [0, 1])"
      ],
      "metadata": {
        "id": "NobnCQ6TRb3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_canc_val, model_canc_bin_gb_clf.predict(X_val_canc), [0, 1])"
      ],
      "metadata": {
        "id": "p-e2KriyRcPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs1 = model_canc_bin_ctree.predict_proba(X_val_canc)[:,1]\n",
        "precision1, recall1, _ = precision_recall_curve(y_canc_val, probs1)\n",
        "probs2 = model_canc_bin_rfc.predict_proba(X_val_canc)[:,1]\n",
        "precision2, recall2, _ = precision_recall_curve(y_canc_val, probs2)\n",
        "probs3 = model_canc_bin_clb.predict_proba(X_val_canc)[:,1]\n",
        "precision3, recall3, _ = precision_recall_curve(y_canc_val, probs3)\n",
        "probs4 = model_canc_bin_gb_clf.predict_proba(X_val_canc)[:,1]\n",
        "precision4, recall4, _ = precision_recall_curve(y_canc_val, probs4)\n",
        "no_skill = len(y_canc_val[y_canc_val==1]) / len(y_canc_val)\n",
        "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "pyplot.plot(recall1, precision1, marker='.', label='Simple')\n",
        "pyplot.plot(recall2, precision2, marker='.', label='RF')\n",
        "pyplot.plot(recall3, precision3, marker='.', label='Bagged')\n",
        "pyplot.plot(recall4, precision4, marker='.', label='Gradient')\n",
        "pyplot.xlabel('Recall')\n",
        "pyplot.ylabel('Precision')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "gI9QlaH4SPTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training models**"
      ],
      "metadata": {
        "id": "skJifb7CS5ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train each model using a 5-fold cross validation\n",
        "2. Retrieve the best parameters\n",
        "3. Plot the feature importances"
      ],
      "metadata": {
        "id": "XHUyChl2SBNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.01, 0.03, 0.05, 0.07, 0.09, 0.1]\n",
        "max_depths = [3,4,5,6,7,8]\n",
        "min_samples_splits = [3,4,5,6,7,8]\n",
        "min_samples_leafs = [1, 2, 3,4,5,6,7,8]\n",
        "criterions = ['gini', 'entropy']"
      ],
      "metadata": {
        "id": "plSZCy1mo4M0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_FP = 100\n",
        "cost_FN = 18\n",
        "param_grid = {\n",
        "    \"criterion\": criterions,\n",
        "    \"max_depth\": max_depths,\n",
        "    \"min_samples_split\": min_samples_splits,\n",
        "    \"min_samples_leaf\": min_samples_leafs,\n",
        "}\n",
        "def cost_fun(y, hat_y):\n",
        "  return np.sum(np.multiply(hat_y, (1 - y)) * cost_FP) + np.sum(np.multiply((1 - hat_y), y) * cost_FN)\n",
        "\n",
        "basemodel = tree.DecisionTreeClassifier()\n",
        "tuned_delayed_model = HalvingGridSearchCV(basemodel, param_grid, scoring=make_scorer(cost_fun, greater_is_better=True), n_jobs=-1, min_resources=\"smallest\", factor=6, cv=5)\n",
        "tuned_delayed_model.fit(X_train, y_train)\n",
        "tuned_delayed_model.best_params_"
      ],
      "metadata": {
        "id": "9utUta8FhjEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = tuned_delayed_model.best_estimator_.feature_importances_\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importances):\n",
        "\tprint(f\"{i} {X_val.columns[i]} Score: {v}\")\n",
        "# plot feature importance\n",
        "pyplot.bar([x for x in range(len(importances))], importances)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "TtUGR2jIIGqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_FP = 100\n",
        "cost_FN = 2\n",
        "def cost_fun(y, hat_y):\n",
        "  return np.sum(np.multiply(hat_y, (1 - y)) * cost_FP) + np.sum(np.multiply((1 - hat_y), y) * cost_FN)\n",
        "\n",
        "basemodel = tree.DecisionTreeClassifier()\n",
        "tuned_canc_model = HalvingGridSearchCV(basemodel, param_grid, scoring=make_scorer(cost_fun, greater_is_better=True), n_jobs=-1, min_resources=\"smallest\", factor=6, cv=5)\n",
        "tuned_canc_model.fit(X_canc_train, y__canc_train)\n",
        "tuned_canc_model.best_params_"
      ],
      "metadata": {
        "id": "1eJaJxUf_5rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = tuned_canc_model.best_estimator_.feature_importances_\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importances):\n",
        "\tprint(f\"{i} {X_val.columns[i]} Score: {v}\")\n",
        "# plot feature importance\n",
        "pyplot.bar([x for x in range(len(importances))], importances)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "SvWAEHs_KHrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GBR=GradientBoostingRegressor()\n",
        "search_grid={'n_estimators':[10, 50, 100],'learning_rate':learning_rates, 'max_depth':max_depths, 'random_state':[10]}\n",
        "tuned_log_delay_model=HalvingGridSearchCV(estimator=GBR,param_grid=search_grid,scoring='neg_mean_squared_error',n_jobs=-1, cv=5)"
      ],
      "metadata": {
        "id": "f6hwt55j_Jlw"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_log_delay_model.fit(X_val,y_log_val)\n",
        "print(f\"best parameters: {tuned_log_delay_model.best_params_}\")"
      ],
      "metadata": {
        "id": "1CcdUSvk_wEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = tuned_log_delay_model.best_estimator_.feature_importances_\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importances):\n",
        "\tprint(f\"{i} {X_val.columns[i]} Score: {v}\")\n",
        "# plot feature importance\n",
        "pyplot.bar([x for x in range(len(importances))], importances)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "XAZz92CbGRgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predictions**"
      ],
      "metadata": {
        "id": "jhNKL2biSMyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Fit the optimal hyperparameters for each model\n",
        "2. Predict them on the test set\n",
        "3. Evaluate them on the test set\n",
        "4. Plot them over time"
      ],
      "metadata": {
        "id": "gItH0zrYRWKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_delayed_model.best_estimator_.fit(X_train, y_train)\n",
        "pred_delay_prob = tuned_delayed_model.best_estimator_.predict(X_test)"
      ],
      "metadata": {
        "id": "iLKdQX3BQCyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'date': test_df['DATES'], 'predicted': pred_delay_prob, 'actual': y_bin_test}\n",
        "delayed_results = pd.DataFrame(data=data)\n",
        "delayed_results['delta'] = delayed_results['actual'] - delayed_results['predicted']"
      ],
      "metadata": {
        "id": "7i2MTWLgQEsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_to_evaluate(y_bin_test, pred_delay_prob, \"Delay Probability evaluation on test set\")"
      ],
      "metadata": {
        "id": "aYxupY4IQMjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(30,5))\n",
        "\n",
        "ax1.plot(delayed_results.groupby('date')['actual'].mean(),data=delayed_results, color='g')\n",
        "ax1.plot(delayed_results.groupby('date')['predicted'].mean(),data=delayed_results, color='b')\n",
        "ax1.set_xlabel('Days of the year')\n",
        "ax1.set_ylabel('Fraction of delays', color='g')\n",
        "ax1.set_xlim(min(delayed_results['date']), max(delayed_results['date']))"
      ],
      "metadata": {
        "id": "fVGlWyn2QHul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_canc_model.best_estimator_.fit(X_canc_train, y__canc_train)\n",
        "pred_canc_prob = tuned_canc_model.predict(X_test_canc)"
      ],
      "metadata": {
        "id": "Guw-KTKfQmql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'date': canc_test_df['DATES'], 'predicted': pred_canc_prob, 'actual': y_canc_test}\n",
        "canc_results = pd.DataFrame(data=data)\n",
        "canc_results['delta'] = canc_results['actual'] - canc_results['predicted']"
      ],
      "metadata": {
        "id": "19YWodk0Q6_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(30,5))\n",
        "\n",
        "ax1.plot(canc_results.groupby('date')['actual'].mean(),data=canc_results, color='g')\n",
        "ax1.plot(canc_results.groupby('date')['predicted'].mean(),data=canc_results, color='b')\n",
        "ax1.set_xlabel('Days of the year')\n",
        "ax1.set_ylabel('Fraction of cancelled/diverted flights', color='g')\n",
        "ax1.set_xlim(min(canc_results['date']), max(canc_results['date']))"
      ],
      "metadata": {
        "id": "W-uWH--tQ8x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_to_evaluate(y_canc_test, pred_canc_prob, \"Cancelled/Diverted Flight Probability evaluation on test set\")"
      ],
      "metadata": {
        "id": "mhho3TQKRBNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model = GradientBoostingRegressor(\n",
        "    max_depth=best_params_reg['max_depth'],\n",
        "    learning_rate= best_params_reg['learning_rate'],\n",
        "    n_estimators= best_params_reg['n_estimators'],\n",
        "\n",
        ")\n",
        "reg_model.fit(X_trained, y_log_train)\n",
        "pred_delay = reg_model.predict(X_test)\n",
        "print(f\"{mean_squared_error(y_log_test, pred_delay)**0.5}\")\n",
        "print(f\"{explained_variance_score(y_log_test, pred_delay)}\")\n",
        "print(f\"{estimate_cost(pred_delay, y_log_test, 100, 1)}\")\n"
      ],
      "metadata": {
        "id": "Jbqi_7apAYK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'date': test_df['DATES'], 'predicted': pred_delay, 'actual': y_log_test}\n",
        "log_delay_results = pd.DataFrame(data=data)\n",
        "log_delay_results['delta'] = log_delay_results['actual'] - log_delay_results['predicted']"
      ],
      "metadata": {
        "id": "Sj2UqU6nsk6b"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(30,5))\n",
        "\n",
        "ax1.plot(log_delay_results.groupby('date')['actual'].mean(),data=log_delay_results, color='g')\n",
        "ax1.plot(log_delay_results.groupby('date')['predicted'].mean(),data=log_delay_results, color='b')\n",
        "ax1.set_xlabel('Days of the year')\n",
        "ax1.set_ylabel('log delays', color='g')\n",
        "ax1.set_xlim(min(log_delay_results['date']), max(log_delay_results['date']))"
      ],
      "metadata": {
        "id": "KIdA2q68uUgz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}